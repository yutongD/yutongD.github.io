<!DOCTYPE HTML>
<html lang="en-US">
<head>
    <title>Yutong Deng</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Template by Dry Themes"/>
    <meta name="keywords" content="HTML, CSS, JavaScript, PHP"/>
    <meta name="author" content="DryThemes"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <link href='http://fonts.googleapis.com/css?family=Roboto:300,400,400i,700,700i,900%7CMontserrat:400,700%7CPT+Serif'
          rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href='css/clear.css'/>
    <link rel="stylesheet" type="text/css" href='css/common.css'/>
    <link rel="stylesheet" type="text/css" href='css/font-awesome.min.css'/>
    <link rel="stylesheet" type="text/css" href='css/carouFredSel.css'/>
    <link rel="stylesheet" type="text/css" href='css/prettyPhoto.css'/>
    <link rel="stylesheet" type="text/css" href='css/sm-clean.css'/>
    <link rel="stylesheet" type="text/css" href='style_mooc.css'/>

    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <script src="js/respond.min.js"></script>
    <![endif]-->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154146632-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-154146632-1');
    </script>

</head>
<body class="post single-post">

<table class="doc-loader">
    <tbody>
    <tr>
        <td>
            <img src="images/ajax-document-loader.gif" alt="Loading...">
        </td>
    </tr>
    </tbody>
</table>

<div class="menu-wraper center-relative">
    <div class="menu-logo center-text">
        <a href="index.html">
            <img src="images/reopenDefaultLogo.png" alt="Reopen"/>
        </a>
    </div>
    <nav id="header-main-menu" class="big-menu">
        <ul class="main-menu sm sm-clean">
            <li>
                <a href="index.html">Home</a>
            </li>
            <li>
                <a href=" ">About</a>
            </li>
        </ul>
    </nav>
    <div class="clear"></div>
</div>


<div class="body-wrapper">
    <div style="top: 0px;left:0px; ">
        <a href="mooc.html"> <span style="position: absolute; top:80px; left: 50px;">
<!--            <img src="images/blossom/left-arrow.png" height="40px" width="40px"> </span> </a>-->
    </div>

    <div class="header-holder center-relative relative">
        <div class="toggle-holder absolute">
            <div id="toggle" style="margin-top: -40px;">
                <a target="_blank" href="index.html"><u>Home</u></a>
<!--                <div class="first-menu-line"></div>-->
<!--                <div class="second-menu-line"></div>-->
<!--                <div class="third-menu-line"></div>-->
            </div>
        </div>
        <div class="clear"></div>
    </div>
    <div id="content" class="site-content center-relative">
        <article style="font-weight: 300">
            <div class="center-relative clear">

                <h1 class="entry-title" style="padding-top: 200px;margin-bottom: 10px">
                    <div id="top" class="single-picture" style="margin-bottom: 50px">
                        <img src="https://img.icons8.com/wired/64/000000/hacker.png" style="width: 80px">
                    </div>
                    SPAMMER DETECTION
                </h1>
                <div class="post-info" style="margin-left: -80px">
                    <div class="cat-links" style="margin-left: 80px">
                        <ul>
                            <li><a href="#">Data Mining</a></li>
                        </ul>
                    </div>
                    <div class="cat-links">
                        <ul>
                            <li><a href="#">Feb 2020 - June 2020</a></li>
                        </ul>
                    </div>
                    <div class="cat-links">
                        <ul>
                            <a href="#">Tensorflow</a>
                        </ul>
                    </div>
                    <br><br>
                </div>
                <div class="entry-content">
                    <div class="content-wrap">
                        <h2 style="margin-left: -85px"><img src="https://img.icons8.com/dusk/64/000000/star-of-bethlehem.png"><b>&nbsp;&nbsp;Quick Links</b></h2>
                        <b>Papers:</b><br>
                        CIKM'20 (<a href="https://arxiv.org/pdf/2008.08692.pdf" target="_blank"><u>PDF</u></a>)&nbsp;&&nbsp;
                        SIGIR'20 (<a href="https://arxiv.org/pdf/2005.00625.pdf" target="_blank"><u>PDF</u></a>)<br>
                        <b>Code:</b><br>
                        <a href="#codes"><u>In-page code sample</u></a><br>
                        <a href="https://github.com/safe-graph/DGFraud" target="_blank"><u>Github:&nbsp;DGFraud ‚Äì a Graph Neural Network (GNN) based toolbox for fraud detection</u></a>
                        <br><br>
                        <h2><b>INTRODUCTION</b></h2>
                        <p>Customers review many different products and services on websites, e.g., Amazon and Yelp.
                            These reviews influence the decisions of other customers. As a result, spammers have become
                            incentivized to publish fake reviews. The BBC, in 2013, reported that nearly 25% of Yelp
                            reviews could be fake [1].
                        </p>
                        <br>
                        <p>However, it can be difficult for users to distinguish fake from honest reviews‚Äîthe average
                            accuracy of
                            three human judges was only 57.33% [2]. It can also be expensive to detect spam by manually
                            reviewing content. So, we need to find technical solutions to automatically spot
                            spammers.</p>
                        <br>
                        <p>
                            To gain an insight into this issue, I conducted research and experiments on the Chinese
                            restaurant review app, <i>Dianping</i>, and assisted in the proposal of a novel framework for
                            online spammer
                            detection. I also led the implementation & comparison of state-of-the-art GNN-based fraud detection models.
                           </p><br>
                        <p><b>The framework I introduce here is the simplified version of our final one.</b></p>
                        <br>
                        <br><br>
                        <h2>THE CHALLENGE</h2>
                        <h4>The reviews themselves are diverse</h4>
                        <p>We can find some obvious abnormal patterns that keep coming up, such as ‚ÄúLove this place,‚Äù
                            ‚ÄúGreat food,‚Äù and
                            ‚ÄúGreat service.‚Äù Actually, there are many additional patterns, such as emotional tendencies,
                            writing habits, or mentioned aspects, which appear in mixed behaviors. It is hard to
                            identify a fixed behavior pattern or decide which characteristic is critical. This makes
                            detecting spammers a challenge. Here is an example:</p>
                        <br><br>
                        <blockquote class="inline-blockquote before-quote">
                            <p style="color: #AA84AA;font-family: 'Century Gothic';font-size: larger;">
                                The <b>food, wine, and service</b> are as <b>good</b> as everyone says.
                                Good <b>noise</b> level, fancy but not too fancy. <b>Price</b> - $200+ for the dinner.
                                <b>Food allergies</b>? - They happily accommodate them in such a seamless way that you
                                don't even notice.
                                <b>Pretentiousness of servers</b> - Not a problem at all.
                                <b>Birthday treat</b>? - Yes! Delicious little truffle cake thing.</p>
                        </blockquote>
                        <br><br><h4>Adversarial actions</h4>
                        <p>Spammers are smart and everywhere. Many hidden fake reviews are mixed with real reviews, and
                            at a glance, there is no difference between them. </p>
                        <br><br>
                        <h2>INSIGHTS</h2>
                        <p>Broadly speaking, review spam falls into two categories. The first is people who are trying
                            to make their own business look good. The second is people who are trying to hurt others.
                            Most of the reviews on <i>Dianping</i> are the latter. After studying raw review content on
                            <i>Dianping</i>, I settled on four influential factors for generating a multi-view schema
                            from
                            scratch:</p>
                        <h4>Emotion</h4>
                        <p>I divided emotions into positive, negative, and normal emotions. The emotional tendency of
                            fake reviews differs between various review platforms. Among all spam reviews, negative
                            reviews appear about three times more often as positive ones.
                        </p>
                        <h4>Aspects</h4>
                        <p>Generally, like the above example, reviews involve particular aspects, such as service, food,
                            price, hygiene, etc.¬†Spammers may tend to focus on particular aspects since they have the
                            same goal.</p>
                        <h4>IP address</h4>
                        <p>An IP can be suspicious or normal. Reviews from the same user or IP address tend to exhibit
                            similar behaviors. Spammers aim to maximize profits and they write as many fake reviews as
                            possible. Therefore, they may use the same user account and IP address to write multiple
                            reviews.
                        </p>
                        <h4>Rank</h4>
                        <p>Many users are accustomed to filtering restaurants by rank, so the number of stars is another
                            important factor to consider. Most spammers rate either one star or five stars to affect the
                            overall ranking as much as possible.
                        </p><br><br>
                        <h2>A MULTI-VIEW GCN FRAMEWORK</h2><br>
                        <p><img src="images/spammer/karate.png" style="margin-left: 150px"></p><br><br>
                        <p>I extracted the above four attributes from reviews. But, it was not enough. I asked
                            myself how to maximize the review feature utility of each reviewer and I thought about graph
                            models. A graph means connections that connect similar nodes together. It enables nodes to
                            use additional useful information from neighbors to obtain better information.</p>
                        <br>
                        <p><b>After repeatedly polishing the idea with team members, we proposed a Multi-View Graph
                            Convolutional Network
                            (GCN) framework for spammer detection.</b></p>
                        <div><img src="images/spammer/framework.png"></div>
                        <br><h4>Neighbor Selection</h4>
                        <p>After experiments on diverse graph datasets, I discovered that GCN performs better when
                            neighboring nodes of the graph are highly similar and the adjacency matrix is neither too
                            dense nor too sparse. </p>
                        <br>
                        <p>More importantly, neighbors sharing similar attributes are critical to GCN‚Äôs performance.
                            Dissimilar neighbors bring noise to representational learning. Therefore, I conducted the
                            similarity-based neighbor selection in multiple dimensions to ensure the similarity between
                            neighbors. For example, reviewers who shared an IP address were connected in the
                            corresponding graph, and reviewers who shared positive emotions were connected in the
                            emotion graph. In this way, we could get high-quality graph networks that were well suited
                            for graph embedding.
                        </p><br>
                        <div class="one_half ">
                            <img src="images/spammer/ip.png">
                            <p style="font-size:medium">Reviewers e1, e3, e5 have posted positive reviews; e4, e5, e6
                                have
                                posted negative reviews; e1, e2 have posted normal reviews.</p>
                        </div>
                        <div class="one_half last ">
                            <img src="images/spammer/emotion.png">
                            <p style="font-size:medium">Reviewers e1, e2, e3 have used IP address 1; e4, e5, e6 have
                                used
                                IP address 2; e2 and e4 have used IP address 3.</p>
                        </div>
                        <div class="clear"></div>
                        <h4>Graph Embedding</h4>
                        <p>From the above steps, we obtained four adjacency matrices. I adopted GCN [3] for graph
                            embedding. Unlike the standard neural network,¬†the nodes in GCN capture the neighbor‚Äôs
                            properties and properties aggregate during the convolution operation. The network tries to
                            learn the embedding for each of the nodes, through mutual sharing of data among the nodes‚Äô
                            neighbors, in an iterative manner until convergence.</p>
                        <br>
                        <p>Furthermore, I made two amazing findings. In my application scenario, the graph network can
                            leverage the
                            relational ties between users with minimal to no external information. In other words, it
                            performs better when nodes' attributes are replaced by an adjacency matrix. This change
                            enables
                            GCN to learn global structural relational information (the whole graph), which further
                            improves the feature utility.</p>
                        <br><h4>Classifier </h4>
                        <p>The last step is to add a Logistic Regression classifier or XGB classifier. The classifier
                            utilizes embedding
                            from four views collectively to output the label of reviewers .
                        </p><br><br>
                        <h2>ADVANTAGES</h2>
                        <p>(1) The framework identifies explicit or latent connections between spammers from reviews
                            characteristic in multiple views, which maximizes review utility and improve the
                            accuracy. </p>
                        <br>
                        <p>(2) The framework exhibits a good performance with only a small amount of training data (1%)
                            which alleviates the costly labeling problem.</p>
                        <br>
                        <p>(3) It is an end-to-end learning framework. It takes labeled nodes and the graph as the input
                            and output the predicted classes of unlabeled nodes.</p>
                        <br>
                        <h4 id="codes">Original code sample:</h4>
                        <div style=" overflow:scroll; width:1000px; height:400px; background-color: #F0EAE9">
                            <div style="font-size: small;line-height:20px;font-family: Consolas;padding-left: 10px">
                                '''<br>
                                This code is due to Yutong Deng (@yutongD), Yingtong Dou (@YingtongDou) and UIC BDSC Lab<br>
                                DGFraud (A Deep Graph-based Toolbox for Fraud Detection)<br>
                                https://github.com/safe-graph/DGFraud<br>
                                '''<br>
                                import tensorflow as tf<br>
                                import argparse<br>
                                import os<br>
                                import sys<br>
                                sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../..')))<br>
                                from algorithms.Player2Vec.Player2Vec import Player2Vec<br>
                                import time<br>
                                from utils.data_loader import *<br>
                                from utils.utils import *<br><br>


                                # os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'<br><br>

                                # init the common args, expect the model specific args<br>
                                def arg_parser():<br>
                                parser = argparse.ArgumentParser()<br>
                                parser.add_argument('--seed', type=int, default=123, help='Random seed.')<br>
                                parser.add_argument('--dataset_str', type=str, default='dblp', help="['dblp','example']")<br>
                                parser.add_argument('--epoch_num', type=int, default=30, help='Number of epochs to train.')<br>
                                parser.add_argument('--batch_size', type=int, default=1000)<br>
                                parser.add_argument('--momentum', type=int, default=0.9)<br>
                                parser.add_argument('--learning_rate', default=0.001, help='the ratio of training set in whole dataset.')<br><br>

                                # GCN args<br>
                                parser.add_argument('--hidden1', default=16, help='Number of units in GCN hidden layer 1.')<br>
                                parser.add_argument('--hidden2', default=16, help='Number of units in GCN hidden layer 2.')<br>
                                parser.add_argument('--gcn_output', default=4, help='gcn output size.')<br><br>

                                args = parser.parse_args()<br>
                                return args<br><br>

                                def set_env(args):<br>
                                tf.reset_default_graph()<br>
                                np.random.seed(args.seed)<br>
                                tf.set_random_seed(args.seed)<br><br>

                                # get batch data<br>
                                def get_data(ix, int_batch, train_size):<br>
                                if ix + int_batch >= train_size:<br>
                                ix = train_size - int_batch<br>
                                end = train_size<br>
                                else:<br>
                                end = ix + int_batch<br>
                                return train_data[ix:end], train_label[ix:end]<br><br>

                                def load_data(args):<br>
                                if args.dataset_str == 'dblp':<br>
                                adj_list, features, train_data, train_label, test_data, test_label = load_data_dblp()<br>
                                node_size = features.shape[0]<br>
                                node_embedding = features.shape[1]<br>
                                class_size = train_label.shape[1]<br>
                                train_size = len(train_data)<br>
                                paras = [node_size, node_embedding, class_size, train_size]<br><br>

                                return adj_list, features, train_data, train_label, test_data, test_label, paras<br><br>

                                def train(args, adj_list, features, train_data, train_label, test_data, test_label, paras):<br>
                                with tf.Session() as sess:<br>
                                adj_data = [normalize_adj(adj) for adj in adj_list]<br>
                                meta_size = len(adj_list)<br>
                                net = Player2Vec(session=sess, class_size=paras[2], gcn_output1=args.hidden1,<br>
                                meta=meta_size, nodes=paras[0], embedding=paras[1], encoding=args.gcn_output)<br><br>

                                sess.run(tf.global_variables_initializer())<br>
                                # net.load(sess)<br><br>

                                t_start = time.clock()<br>
                                for epoch in range(args.epoch_num):<br>
                                train_loss = 0<br>
                                train_acc = 0<br>
                                count = 0<br>
                                for index in range(0, paras[3], args.batch_size):<br>
                                batch_data, batch_label = get_data(index, args.batch_size, paras[3])<br>
                                loss, acc, pred, prob = net.train(features, adj_data, batch_label,<br>
                                batch_data, args.learning_rate,<br>
                                args.momentum)<br><br>

                                print("batch loss: {:.4f}, batch acc: {:.4f}".format(loss, acc))<br>
                                # print(prob, pred)<br><br>

                                train_loss += loss<br>
                                train_acc += acc<br>
                                count += 1<br>
                                train_loss = train_loss / count<br>
                                train_acc = train_acc / count<br>
                                print("epoch{:d} : train_loss: {:.4f}, train_acc: {:.4f}".format(epoch, train_loss, train_acc))<br>
                                # net.save(sess)<br><br>

                                t_end = time.clock()<br>
                                print("train time=", "{:.5f}".format(t_end - t_start))<br>
                                print("Train end!")<br><br>

                                test_acc, test_pred, test_probabilities, test_tags = net.test(features, adj_data, test_label,test_data)<br>
                                print("test acc:", test_acc)<br><br>

                                if __name__ == "__main__":<br>
                                args = arg_parser()<br>
                                set_env(args)<br>
                                adj_list, features, train_data, train_label, test_data, test_label, paras = load_data(args)<br>
                                train(args, adj_list, features, train_data, train_label, test_data, test_label, paras)<br>

                            </div>
                        </div>

                         <!--<h2>TAKEAWAYS</h2>
                        <p>Before I went to the United States to do lab research in summer 2019, I was inexperienced in
                            scientific research and had never gone abroad alone. These were challenges for me.
                            Fortunately, I believe that the biggest challenges are made up of small challenges, and
                            there is no reason to give up when opportunities come my way. I finished the preparation for
                            going to the United States by myself. At the beginning of my
                            research life, I always failed to keep up with group discussions, which made me feel
                            frustrated and stressed.
                        </p>
                        <br>
                        <p>A PhD student told me that when you read 100 papers in this field, you are just getting
                            started, and it is the accumulation of the amount you read that is essential. After reading
                            many papers and documents on related experiments, I developed a general understanding of the
                            graph network and of what "good" ideas are. On this basis, I started to put forward my ideas
                            and independently conduct research. Gradually I was able to contribute to our team.</p>
                        <br>
                        <p>I also saw significant improvements in my communication skills by talking frequently with
                            native English speakers and learning more idiomatic expressions in two months than in my
                            entire life previously.</p>
                        <br>
                        <p>
                            A whole year of research has an important influence on my thoughts and career. My achievements make me aware that the challenges themselves are not
                            difficult. The real challenge for me is to constantly adjust my attitude towards them and keep going.
                            And I have done it!
                        </p>
                        <br>
                        <p> I ENJOYED THIS CHALLENGE! üòä </p><br>-->
                        <h2>REFERENCE</h2>
                        <p>[1] http://www.bbc.com/news/technology-24299742</p>
                        <p>[2] Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Hancock. 2011. Finding deceptive
                            opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of
                            the Association for Computational Linguistics: Human Language Technologies, pages
                            309‚Äì319.</p>
                        <p>[3] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
                            convolutional networks. In 5th International Conference on Learning Representations,
                            ICLR. </p><br><br>
                        <a href="#top"><u><b><font size="6">Back To Top‚Üë</font></b></u></a>
                        <a href="mooc.html"><img src="images/blossom/circled-right.png" style="margin-left: 1000px;padding-top: -50px"></a><br><br>
                </div>
                </div>
            </div>
        </article>
    </div>
</div>

<footer class="footer">
    <div class="content-970 center-relative">
        <ul>
            <li class="copyright-footer">
                2019 ¬© Yutong Deng
            </li>
        </ul>
    </div>
</footer>

<!-- End .body-border -->


<!--Load JavaScript-->
<script src="js/jquery.js"></script>
<script src='js/jquery.fitvids.js'></script>
<script src='js/jquery.smartmenus.min.js'></script>
<script src='js/isotope.pkgd.js'></script>
<script src='js/imagesloaded.pkgd.js'></script>
<script src='js/isotope.pkgd.js'></script>
<script src='js/jquery.carouFredSel-6.0.0-packed.js'></script>
<script src='js/jquery.mousewheel.min.js'></script>
<script src='js/jquery.touchSwipe.min.js'></script>
<script src='js/jquery.easing.1.3.js'></script>
<script src='js/imagesloaded.pkgd.js'></script>
<script src='js/jquery.prettyPhoto.js'></script>
<script src='js/jquery.nicescroll.min.js'></script>
<script src='js/main.js'></script>
</body>
</html>
